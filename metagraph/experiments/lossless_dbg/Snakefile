import os
import json
import platform
import warnings
from common import *
import re
from functools import partial

# Warning: requires at least gzip 1.6 --k
#
# def save_info(filename,final_filesize):
# 	json.dump({"compressed size": final_filesize},open(filename,"w"))
#
configfile: "config.yaml"
#
# rule all:
# 	input: expand("results/{run_id}.{compressor}.size.json",run_id=config["samples"],compressor=config["compressors"])
#
#
#bsc_command = "/Users/janstudeny/Library/Mobile Documents/com~apple~CloudDocs/Active projects/Master Thesis/playground/HARC/src/libbsc/bsc"

#ruleorder: download_fastq_gz > download_fastq > download_sra > bscunzip > gunzip
def get_versioned_methods(version):
    methods_raw = ["logan","mccortex","harc","darrc"] + [f"path_encoder_{version}"]
    methods = methods_raw + [method + " [compressor: gzip]" for method in methods_raw if not method=="harc"]
    return methods


def get_input_size(wildcards,input):
    try:
        input_file = str(input)
        size = 0
        if (os.path.isfile(input_file)):
            size = os.path.getsize(input_file)
        else:
            size = sum(os.path.getsize(f) for f in os.listdir(input_file) if os.path.isfile(f))
        return size//1000000 + 1000
    except:
        return 40000

def get_ram_usage_per_core(wildcards,input,threads,attempt,multiplier=4):
    return int(get_input_size(wildcards,input)/threads*(2**(attempt-1))*2*multiplier) # *2 as cores is half amount of threads

def get_time_in_min(wildcards,input,threads,attempt,speed_in_mbps=1):
    return max(int(get_input_size(wildcards,input)/speed_in_mbps*(2**(attempt-1))/60+1),240) # +1 to round up

path_encoder_version = "v3"
methods = get_versioned_methods(path_encoder_version)

# minitest
format = "artificial_chr10-subsample_size-{subsample_size}-read_length-{read_length}-coverage-{coverage}-error-{error}"

rule all:
    input:

# rule all:
#     #input: expand("compressors_results/{compressors}/SRR554369_1/",compressors=config["compressors"])
#     # input: expand("compressors_results/{compressor}/artificial_chr10_{subsample_size}_{read_lenght}_{coverage}",\
#     #             subsample_size=[10000],\
#     #             read_lenght=[100],\
#     #             coverage=[100],\
#     #             compressor=methods)
#     input: expand(f"plots/comparison/{path_encoder_version}/" + format + ".html",\
#                 subsample_size=[100000],\
#                 read_length=[100],\
#                 coverage=[100],\
#                 error=[0.03])
#     shell:
#         "#rmate {input}"

def get_folders(wildcards):
    return {**{ method : 'compressors_results/' + method + '/{wildcards.run_id}'.format(wildcards=wildcards) for method in get_versioned_methods(wildcards.version)},
            'fasta': f'genomic_data/fastq/{wildcards.run_id}.fasta'}

localrules: generate_plots
rule generate_plots:
    input: unpack(get_folders)
    output: "plots/comparison/{version}/{run_id}.html"
    script: "plotting.py"

path_encoder_versions = [f"path_encoder_{version}" for version in ["v3","v4","v5"]]

def get_pe_folders(wildcards):
    return {**{ method : 'compressors_results/' + method + '/{wildcards.run_id}'.format(wildcards=wildcards) for method in path_encoder_versions},
            'fasta': f'genomic_data/fastq/{wildcards.run_id}.fasta'}

localrules: generate_pe_plots
rule generate_pe_plots:
    input: unpack(get_pe_folders)
    output: "plots/pe_comparison/{run_id}.html"
    script: "plotting.py"



def outputs_parameter_range(wildcards):
    output = {}
    for i in range(int(wildcards.low),int(wildcards.high)):
        name = f"path_encoder_{wildcards.version} ({wildcards.parameter}: {i})"
        output[name] = f'compressors_results/{name}/{wildcards.run_id}'
    output['fasta'] = f'genomic_data/fastq/{wildcards.run_id}.fasta'
    return output

localrules: benchmark_size_parameter
rule benchmark_size_parameter:
    input: unpack(outputs_parameter_range)
    output: "plots/pe_parameter_comparison/{version}/{parameter}_{low}_{high}/{run_id}.html"
    params:
        sort = False
    script: "plotting.py"

def get_needed_files(wildcards):
    output = {}
    for i in range(int(wildcards.low),int(wildcards.high)):
        output[str(i)] = f"benchmarks/{wildcards.compressor} ({wildcards.parameter}: {i})/{wildcards.run_id}.tsv"
    return output

localrules: benchmark_parameter
rule benchmark_parameter:
    input: unpack(get_needed_files)
    output: "plots/benchmarks/{compressor}/{parameter}_{low}_{high}/{run_id}.html"
    script: "plotting_parameter.py"

# ToDo add re.escape to escape the names
wildcard_constraints:
    version = r'v\d+',
    parameter = '[a-z\-]+',
    value = r'[\w]+',
    gzip_download_run_id = '|'.join(config['samples_fastq_gz']),
    fastq_download_run_id = '|'.join(config['samples_fastq']),
    download_run_id = '|'.join(config['fqsqueezer_samples']),
    sra_download_run_id = '|'.join(config["samples"]),
    parameters = '[^[]+',
    variable_parameters = r'.*\[.*\]'


localrules: download_sra
rule download_sra:
    output: "genomic_data/sra/{sra_download_run_id}.sra"
    shell: lambda wildcards: f"wget {config['samples'][wildcards.sra_download_run_id]}" + " -O {output}"

#"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR554/SRR554369/SRR554369_2.fastq.gz"
def download_url(id):
    return f"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/{id[:6]}/{id[:9]}/{id}.fastq.gz"

rule download_raw_run_id:
	output: "genomic_data/fastq/{download_run_id}.fastq.gz"
	params:
		url=lambda wildcards: download_url(wildcards.download_run_id)
	shell: "wget {params.url} -O {output}"

localrules: download_fastq_gz
rule download_fastq_gz:
	output: "genomic_data/fastq/{gzip_download_run_id}.fastq.gz"
	params:
		url=lambda wildcards: (config['samples_fastq_gz'][wildcards.gzip_download_run_id])
	shell: "wget {params.url} -O {output}"

localrules: gunzip_fastq
rule gunzip_fastq:
    input: "{file}.fastq.gz"
    output: "{file}.fastq"
    shell: "gzip -d --keep {input}"

localrules: download_fastq
rule download_fastq:
	output: "genomic_data/fastq/{fastq_download_run_id}.fastq"
	params:
		url=lambda wildcards: (config['samples_fastq'][wildcards.fastq_download_run_id])
	shell: "wget {params.url} -O {output}"

localrules: encode_on_human_graph
rule encode_on_human_graph:
    input: "compressors_results/path_encoder_{version} (graph: genomic_data/human.dbg)/{read_id}"
    output: directory("shortcuts/encoded_on_human/{version}/{read_id}")
    shell:"""
        mkdir -p "$(dirname '{output}')"
        ln -s "$(realpath '{input}')" '{output}'
    """

localrules: encode_build_graph
rule encode_build_graph:
    input: "genomic_data/graphs/({parameters})/{read_id}.dbg",
        "compressors_results/path_encoder_{version} (graph: genomic_data/graphs/({parameters})/{read_id}.dbg, {parameters})/{read_id}"
    output: directory("shortcuts/encode_build_graph/{version}/({parameters})/{read_id}")
    shell:"""
        mkdir -p "$(dirname '{output}')"
        ln -s "$(realpath '{input}')" '{output}'
    """

# rule bscunzip:
#     input: "{file}.bsc"
#     output: "{file}"
#     shell: "'" + bsc_command + "' d {input} {output}"
#


def download_only_dataset(wildcards):
    result = []
    for run_id in config[wildcards.dataset]:
        result.append(f"genomic_data/fastq/{run_id}.fastq.gz")
    return result

localrules: download_only_dataset
rule download_only_dataset:
    input: unpack(download_only_dataset)
    output: "download_dataset/{dataset}/done.txt"


def all_compressed_dataset(wildcards):
    result = []
    for run_id in config[wildcards.dataset]:
        result += [f"compressors_results/{method}/{run_id}" for method in get_versioned_methods(wildcards.version)]
    return result

localrules: compress_dataset
rule compress_dataset:
    input: unpack(all_compressed_dataset)
    output: "compress_dataset/{version}/{dataset}/done.txt"
    shell: "touch '{output}'"



localrules: sra_to_fastq
rule sra_to_fastq:
    input: "genomic_data/sra/{run_id}.sra"
    output: "genomic_data/fastq/{run_id}.fastq"
    shell: "fastq-dump {input} -O fastq"

localrules: generate_fastq
rule generate_fastq:
    input: "genomic_data/human_chromosome_10_actg_only.fasta"
    output: "genomic_data/fastq/" + format + ".fasta"
    shell: "path_encoder_toolbox sample "
           "--reference {input} "
           "--subsample-size {wildcards.subsample_size} "
           "--read-length {wildcards.read_length} "
           "--coverage {wildcards.coverage} "
           "--error-probability {wildcards.error} "
           "--output {output}"


rule run_logan:
    input: "genomic_data/fastq/{run_id}.fasta"
    output: directory("compressors_results/logan/{run_id}")
    threads: 8
    resources:
        cores=lambda wildcards,threads: threads//2,
        mem_per_core=get_ram_usage_per_core,
        time=partial(get_time_in_min,speed_in_mbps=5) # initial guess
    shell: """
        mkdir -p {output}
        LOGAN IndexAndRoute {threads} {threads} {output}/graph {input}
    """

competitiors_threads = 2


def decode_parameters(wildcards,threads,threads_name="threads"):
    parameters = {}
    for parameter_str in wildcards.parameters.split(", "):
        name, value = parameter_str.split(": ",1)
        parameters[name] = value
    if "threads" not in parameters.keys():
        parameters[threads_name] = threads
    return parameters

def encode_parameters(parameters):
    return " ".join([f"--{name} '{value}'" for name, value in parameters.items()])


rule path_encoder_variable_param_specialization:
    input: unpack(fix_variable_parameters)
    output: directory(var_parameters)
    shell: "mkdir '{output}'"

# Todo grab number of threads from parameters

rule path_encoder_param_specialization:
    input: "genomic_data/fastq/{run_id}.fasta"
    benchmark:
           "benchmarks/path_encoder_{version} ({parameters})/{run_id}.tsv"
    output: directory("compressors_results/path_encoder_{version} ({parameters})/{run_id}")
    threads: 30
    priority: 1
    log: "logs/path_encoder_{version} ({parameters})/{run_id}.log"
    params:
        profiler_file = lambda wildcards: f"profiler/path_encoder_{wildcards.version} ({wildcards.parameters})/{wildcards.run_id}.prof",
        shell_parameters = lambda wildcards,threads: encode_parameters(decode_parameters(wildcards,threads))
    resources:
        cores=lambda wildcards,threads: threads//2,
#        mem_per_core=partial(get_ram_usage_per_core,multiplier=40),
        mem_per_core=lambda wildcards, threads: 300_000 * 2 // threads,
        time=partial(get_time_in_min,speed_in_mbps=0.7) # 0.8 is more accurate
    shell: """
        mkdir -p '{output}'
        mkdir -p "$(dirname '{params.profiler_file}')"
        CPUPROFILE='{params.profiler_file}' path_encoder_toolbox_{wildcards.version} compress\
                                      --input '{input}' --output '{output}'\
                                      {params.shell_parameters}  2> >(tee '{log}' >&2)'
    """


rule build_graph:
    input: "genomic_data/fastq/{run_id}.fasta"
    output: "genomic_data/graphs/({parameters})/{run_id}.dbg"
    threads: 30
    params:
        shell_parameters = lambda wildcards,threads: encode_parameters(decode_parameters(wildcards,threads,threads_name="parallel")),
        output_base = lambda wildcards,output: str(output).rsplit(".",1)[0]
    shell: "~/projects2014-metagenome/metagraph/build/Release/metagengraph build -v {params.shell_parameters} '{input}' -o '{params.output_base}' {input}"


# rule path_encoder_default:
#     input: "genomic_data/fastq/{run_id}.fasta"
#     benchmark:
#            "benchmarks/path_encoder_{version}/{run_id}.tsv"
#     output: directory("compressors_results/path_encoder_{version}/{run_id}")
#     threads: 60
#     priority: 1
#     params:
#         profiler_file = "profiler/path_encoder_{version}/{run_id}.prof"
#     resources:
#         cores=lambda wildcards,threads: threads//2,
#         mem_per_core=partial(get_ram_usage_per_core,multiplier=15),
#         time=partial(get_time_in_min,speed_in_mbps=0.7) # 0.8 is more accurate
#     shell: """
#         mkdir -p '{output}'
#         CPUPROFILE='{params.profiler_file}' path_encoder_toolbox_{wildcards.version}_gprof compress\
#                                       --statistics '{output}'/statistics.json\
#                                       --input '{input}' --output '{output}'\
#                                       --threads '{threads}'
#     """

rule mccortex:
    input: "genomic_data/fastq/{mc_run_id}.fasta"
    output: directory("compressors_results/mccortex/{mc_run_id}")
    threads: competitiors_threads
    resources:
        cores=lambda wildcards,threads: threads//2,
        mem_per_core=get_ram_usage_per_core,
        time=partial(get_time_in_min,speed_in_mbps=5) # inital guess
    run:
        shell("mkdir -p '{output}'")
        if wildcards.mc_run_id not in ["SRR870667_1","SRR327342_1"]:
            shell("""
        mccortex31 build --sample {wildcards.mc_run_id} --seq {input} -k 21 {output}/graph.ctx
        mccortex31 thread --seq {input} --out {output}/graph.ctp {output}/graph.ctx
    """)

rule method_gzip:
    input: "compressors_results/{compressor}/{run_id}"
    output: directory("compressors_results/{compressor} [compressor: gzip]/{run_id}")
    threads: competitiors_threads
    resources:
        cores=lambda wildcards,threads: threads//2,
        mem_per_core=get_ram_usage_per_core,
        time=partial(get_time_in_min,speed_in_mbps=10) # inital guess
    shell: """
        mkdir -p '{output}'
        cp -r '{input}'/* '{output}' || :
        gzip -r '{output}'
        sleep 0.1
    """

localrules: fastq_to_fasta
rule fastq_to_fasta:
    input: "genomic_data/fastq/{run_id}.fastq"
    output: "genomic_data/fastq/{run_id}.fasta"
    threads: competitiors_threads
    resources:
        cores=lambda wildcards,threads: threads//2,
        mem_per_core=get_ram_usage_per_core
    shell: "seqtk seq -A '{input}' > '{output}'"

rule run_harc_compress:
    #shadow: "minimal"
    # input: "genomic_data/fastq/{run_id}.fast{suffix,[aq]}"
    input: "genomic_data/fastq/{run_id}.fasta"
    output: "harc/{run_id}.fasta.harc"
    threads: 2
    resources:
        cores=lambda wildcards,threads: threads//2,
        mem_per_core=get_ram_usage_per_core,
        time=partial(get_time_in_min,speed_in_mbps=2) # inital guess
    run:
        shell("""
        rm -rf genomic_data/fastq/output
        INPUT=$(realpath {input})
        pushd ~/HARC
        harc -c $INPUT
        popd
        mv {input}.harc $(dirname "{output}")
        """)
        #mv genomic_data/fastq/{wildcards.run_id}.harc $(dirname "{output}")

rule harc_expand_archive:
    input: "harc/{run_id}.fasta.harc"
    output: directory("compressors_results/harc/{run_id}")
    threads: 2
    resources:
        cores=lambda wildcards,threads: threads//2,
        mem_per_core=get_ram_usage_per_core,
        time=partial(get_time_in_min,speed_in_mbps=2) # inital guess
    shell: """
        mkdir -p {output}
        tar xf {input} -C {output}
    """

rule compress_darrc:
    input: "genomic_data/fastq/{run_id}.fasta"
    output: directory("compressors_results/darrc/{run_id}")
    shadow: "shallow"
    threads: 2
    resources:
        cores=lambda wildcards,threads: threads//2,
        mem_per_core=get_ram_usage_per_core,
        time=partial(get_time_in_min,speed_in_mbps=2) # inital guess
    shell: """
        mkdir -p {output}
        ulimit -n 2048
        darrc -c -1 {input} -g ggg -m sss
        mv ggg.graph.darrc {output}
        mv sss.meta.darrc {output}
    """
		#save_info(output[0],os.path.getsize("ggg.graph.darrc") + os.path.getsize("sss.meta.darrc"))
#
# rule compress_minicom:
# 	input: "fastq/{run_id}.fastq","compressors/minicom"
# 	output: "results/{run_id}.minicom.size.json"
# 	shadow: "full"
# 	run:
# 		shell("compressors/minicom -r {input}")
# 		save_info(output[0],os.path.getsize("{run_id}_comp.minicom"))
#
#
rule build_bft:
	output: "tmp/bft_done.txt"
	shadow: "shallow"
	shell: """
        brew tap traildb/judy
		brew install jemalloc judy p7zip || true
		git clone https://github.com/GuillaumeHolley/BloomFilterTrie
        cd BloomFilterTrie
		./configure --prefix=/cluster/home/studenyj
		make
		make install
        mkdir -p tmp
		touch tmp/bft_done.txt
        """

rule build_darrc:
	input: "tmp/bft_done.txt"
	output: "/cluster/home/studenyj/bin/darrc"
	shadow: "shallow"
	shell: """
		brew install jemalloc p7zip
		git clone https://github.com/GuillaumeHolley/DARRC
        cd DARRC
		./configure --prefix=/cluster/home/studenyj
        make
		make install
    """
#
# rule build_minicom:
# 	output: "compressors/minicom"
# 	shadow: "full"
# 	shell: """
# 		git clone https://github.com/yuansliu/minicom.git
# 		cd minicom
# 		#libbsc
# 		rm -rf src/tools/libbsc
# 		git clone https://github.com/jendas1/libbsc.git src/tools/libbsc
# 		pushd src/tools/libbsc
# 		make CC={cpp_compiler}
# 		popd
# 		cp src/tools/libbsc/bsc ./
#
# 		#p7zip
# 		rm -rf src/tools/p7zip_16.02
# 		wget https://sourceforge.net/projects/p7zip/files/p7zip/16.02/p7zip_16.02_x86_linux_bin.tar.bz2 -P src/tools/
# 		tar -jxvf src/tools/p7zip_16.02_x86_linux_bin.tar.bz2 -C src/tools/
# 		rm -rf src/tools/p7zip_16.02_x86_linux_bin.tar.bz2
# 		cp src/tools/p7zip_16.02/bin/7z ./
# 		cp src/tools/p7zip_16.02/bin/7z.so ./
#
# 		# a pseudo file ‘config.h’ must be provided
# 		cd src
# 		make clean
# 		cp config_pseudo.h config.h
# 		make minicomsg CC={cpp_compiler}
# 		# echo -e "\033[40;37m Compress single reads is OK. \033[0m"
#
# 		echo "#define ORDER" >> config.h
# 		echo "int cmpcluster3(const void *a_, const void *b_);" >> config.h
# 		make minicompe CC={cpp_compiler}
# 		# echo -e "\033[40;37m Compress paired-end reads is OK. \033[0m"
#
# 		make decompress CC={cpp_compiler}
# 		# echo -e "\033[40;37m Decompress is OK. \033[0m"
#
# 		cd ..
# 		cp src/decompress ./
# 		chmod +x minicom
# 		cd ..
# 		cp minicom/minicom {output}
#         """
